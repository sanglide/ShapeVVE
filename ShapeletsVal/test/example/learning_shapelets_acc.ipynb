{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T01:11:38.827407Z",
     "start_time": "2025-09-11T01:11:34.669908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ShapeletsVal.DataLoader.loadDataFromAfrr import get_UEA_dataset\n",
    "\n",
    "# 1.加载数据集\n",
    "DATASET_PATH = \"../../../../data_files/Multivariate2018_arff/Multivariate_arff\"\n",
    "x_train, y_train, x_val, y_val, x_test, y_test=get_UEA_dataset(DATASET_PATH,\"SpokenArabicDigits\")\n",
    "print(f\"x_train: {x_train.shape}, y_train: {y_train.shape}, x_val: {x_val.shape}, y_val: {y_val.shape}, x_test: {x_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "in_channels=len(x_train[0])\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_val = torch.from_numpy(x_val)\n",
    "x_test = torch.from_numpy(x_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_val = torch.from_numpy(y_val)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "print(f'n_classes: {n_classes}')"
   ],
   "id": "d95d162dcf0bd260",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - Train: (6599, 13, 93), Val: (440, 13, 93), Test: (1759, 13, 93)\n",
      "x_train: (6599, 13, 93), y_train: (6599,), x_val: (440, 13, 93), y_val: (440,), x_test: (1759, 13, 93), y_test: (1759,)\n",
      "n_classes: 10\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T01:12:12.274135Z",
     "start_time": "2025-09-11T01:11:41.054242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ShapeletsVal.PredictorModel.LearningShapeletsOrigin import LearningShapelets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# 假设我们有一些时间序列数据X和标签y\n",
    "# X的形状应为(n_samples, in_channels, len_ts)\n",
    "# 这里我们生成一些随机数据作为示例\n",
    "# 检查输入数据\n",
    "print(torch.isnan(x_train).any())  # 检查是否有 nan\n",
    "print(torch.isinf(x_train).any())  # 检查是否有 inf\n",
    "x_train=torch.nan_to_num(x_train, nan=0.0)\n",
    "x_val=torch.nan_to_num(x_val, nan=0.0)\n",
    "x_test=torch.nan_to_num(x_test, nan=0.0)\n",
    "\n",
    "\n",
    "# 编码标签\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# 定义模型参数\n",
    "shapelets_size_and_len = {15: 10,  30: 10}  # 3个block，分别有5个长度为10、20、30的shapelets\n",
    "in_channels = x_train.shape[1]\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "dist_measure = 'euclidean'  # 可以选择 'euclidean', 'cross-correlation', 或 'cosine'\n",
    "\n",
    "model=LearningShapelets(\n",
    "    shapelets_size_and_len=shapelets_size_and_len,  # 增加shapelet数量\n",
    "    loss_func=nn.CrossEntropyLoss(),\n",
    "    in_channels=in_channels,  # 确保与数据实际通道数一致\n",
    "    num_classes=num_classes,\n",
    "    dist_measure='euclidean',\n",
    "    verbose=1,\n",
    "    to_cuda=torch.cuda.is_available(),\n",
    "    # k=2,           # 增大最近邻数量\n",
    "    # l1=0.01,        # 降低距离损失权重\n",
    "    # l2=0.001,        # 降低相似性损失权重\n",
    ")\n",
    "\n",
    "# 使用更强的优化器\n",
    "optimizer = Adam(model.model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "losses = model.fit(\n",
    "    x_train,\n",
    "    y_train_encoded,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 在测试集上评估\n",
    "model.model.eval()  # 设置模型为评估模式\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 获取shapelets\n",
    "shapelets = model.get_shapelets()\n",
    "print(f\"Shapelets shape: {shapelets.shape}\")\n",
    "\n",
    "# 获取线性层权重\n",
    "weights, biases = model.get_weights_linear_layer()\n",
    "print(f\"Linear layer weights shape: {weights.shape}\")\n",
    "print(f\"Linear layer biases shape: {biases.shape}\")\n",
    "\n",
    "# 可视化训练损失\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if isinstance(losses, tuple):\n",
    "    # 如果有多个损失（使用正则化时）\n",
    "    losses_ce, losses_dist, losses_sim = losses\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(losses_ce)\n",
    "    plt.title('Cross-Entropy Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(losses_dist)\n",
    "    plt.title('Distance Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(losses_sim)\n",
    "    plt.title('Similarity Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "\n",
    "else:\n",
    "    # 只有交叉熵损失\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Cross-Entropy Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c269684a759338b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.263418197631836:   2%|▏         | 9/500 [00:30<28:05,  3.43s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 52\u001B[0m\n\u001B[1;32m     50\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m500\u001B[39m\n\u001B[1;32m     51\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m32\u001B[39m\n\u001B[0;32m---> 52\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_train_encoded\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m     58\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;66;03m# 在测试集上评估\u001B[39;00m\n\u001B[1;32m     61\u001B[0m model\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()  \u001B[38;5;66;03m# 设置模型为评估模式\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Code/ShapeletsVal/ShapeletsVal/PredictorModel/LearningShapeletsOrigin.py:926\u001B[0m, in \u001B[0;36mLearningShapelets.fit\u001B[0;34m(self, X, Y, epochs, batch_size, shuffle, drop_last, **kwargs)\u001B[0m\n\u001B[1;32m    924\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, (x, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_dl):\n\u001B[1;32m    925\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_regularizer:\n\u001B[0;32m--> 926\u001B[0m         current_loss_ce \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    927\u001B[0m         losses_ce\u001B[38;5;241m.\u001B[39mappend(current_loss_ce)\n\u001B[1;32m    928\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/Code/ShapeletsVal/ShapeletsVal/PredictorModel/LearningShapeletsOrigin.py:828\u001B[0m, in \u001B[0;36mLearningShapelets.update\u001B[0;34m(self, x, y)\u001B[0m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;124;03mPerforms one gradient update step for the batch of time series and corresponding labels y.\u001B[39;00m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;124;03m@param x: the batch of time series\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    824\u001B[0m \u001B[38;5;124;03m@rtype: float\u001B[39;00m\n\u001B[1;32m    825\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    827\u001B[0m \u001B[38;5;66;03m# 这里直接调用 model(x) 是做前向传播 forward\u001B[39;00m\n\u001B[0;32m--> 828\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[38;5;66;03m# 这里使用的是demo中定义的分类交叉熵损失函数\u001B[39;00m\n\u001B[1;32m    830\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_func(y_hat, y)\n",
      "File \u001B[0;32m~/miniconda3/envs/shapelets-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/shapelets-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Code/ShapeletsVal/ShapeletsVal/PredictorModel/LearningShapeletsOrigin.py:656\u001B[0m, in \u001B[0;36mLearningShapeletsModel.forward\u001B[0;34m(self, x, optimize)\u001B[0m\n\u001B[1;32m    648\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, optimize\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124macc\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    649\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    650\u001B[0m \u001B[38;5;124;03m    Calculate the distances of each time series to the shapelets and stack a linear layer on top.\u001B[39;00m\n\u001B[1;32m    651\u001B[0m \u001B[38;5;124;03m    @param x: the time series data\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    654\u001B[0m \u001B[38;5;124;03m    @rtype: tensor(float) of shape (num_samples, num_classes)\u001B[39;00m\n\u001B[1;32m    655\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 656\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshapelets_blocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    657\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m optimize \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124macc\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    658\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(x)\n",
      "File \u001B[0;32m~/miniconda3/envs/shapelets-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/shapelets-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Code/ShapeletsVal/ShapeletsVal/PredictorModel/LearningShapeletsOrigin.py:418\u001B[0m, in \u001B[0;36mShapeletsDistBlocks.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    416\u001B[0m out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat)\u001B[38;5;241m.\u001B[39mcuda() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_cuda \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtensor([], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat)\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[0;32m--> 418\u001B[0m     block_out \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    419\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:  \u001B[38;5;66;03m# 检查是否为空张量\u001B[39;00m\n\u001B[1;32m    420\u001B[0m         out \u001B[38;5;241m=\u001B[39m block_out\n",
      "File \u001B[0;32m~/miniconda3/envs/shapelets-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/shapelets-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Code/ShapeletsVal/ShapeletsVal/PredictorModel/LearningShapeletsOrigin.py:78\u001B[0m, in \u001B[0;36mMinEuclideanDistBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     62\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39munfold(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshapelets_size, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# calculate euclidean distance\u001B[39;00m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m# print(f'x.shape: {x.shape} shapelets.shape: {self.shapelets.shape}')\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;66;03m# x = torch.cdist(x, self.shapelets, p=2)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# hard min compared to soft-min from the paper\u001B[39;00m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m# x, _ = torch.min(x, 3)\u001B[39;00m\n\u001B[0;32m---> 78\u001B[0m x\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_shapelet_distances\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshapelets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;66;03m# print(x.shape)\u001B[39;00m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/Desktop/Code/ShapeletsVal/ShapeletsVal/PredictorModel/LearningShapeletsOrigin.py:100\u001B[0m, in \u001B[0;36mMinEuclideanDistBlock.compute_shapelet_distances\u001B[0;34m(self, input_tensor, shapelets)\u001B[0m\n\u001B[1;32m     98\u001B[0m input_tensor_flatten\u001B[38;5;241m=\u001B[39minput_tensor_flatten\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# print(f'{input_tensor_flatten.shape}')\u001B[39;00m\n\u001B[0;32m--> 100\u001B[0m input_tensor_flatten \u001B[38;5;241m=\u001B[39m \u001B[43minput_tensor_flatten\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 确保输入是float32\u001B[39;00m\n\u001B[1;32m    101\u001B[0m shapelets \u001B[38;5;241m=\u001B[39m shapelets\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)  \u001B[38;5;66;03m# 确保shapelets也是float32\u001B[39;00m\n\u001B[1;32m    104\u001B[0m distances \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcdist(\n\u001B[1;32m    105\u001B[0m     input_tensor_flatten,  \u001B[38;5;66;03m# [ dimension, n_sample*window_num, window_length]\u001B[39;00m\n\u001B[1;32m    106\u001B[0m     shapelets,  \u001B[38;5;66;03m# [ dimension,num_shapelets, window_length]\u001B[39;00m\n\u001B[1;32m    107\u001B[0m     p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m  \u001B[38;5;66;03m# 欧式距离\u001B[39;00m\n\u001B[1;32m    108\u001B[0m )  \u001B[38;5;66;03m# 结果形状: [dimension,n_sample*window_num, num_shapelets]\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "445762d17f7ac9f3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
